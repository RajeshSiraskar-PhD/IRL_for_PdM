{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm # Progress bar\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# RL libraries\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# CH-AI Imitation libraries\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "\n",
    "# Custom libraries\n",
    "# from milling_tool_env import MillingTool\n",
    "from utilities import downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'PHM_C01.csv'\n",
    "WEAR_THRESHOLD = 0.12\n",
    "SAMPLING_RATE = 1\n",
    "ADD_NOISE = 0\n",
    "MILLING_OPERATIONS_MAX = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_wear_data(data_file, wear_threshold, normalize=False, add_noise=False, sampling_rate=1):\n",
    "    ## Read data\n",
    "    df_raw = pd.read_csv(data_file)\n",
    "\n",
    "    df = downsample(df_raw, sampling_rate)\n",
    "\n",
    "    # Reset index as the downsampling disturbs the index and then PPO.learn() fails. Gives a \"Key error\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    n_points = len(df.index)\n",
    "\n",
    "    # 1. Add white noise for robustness\n",
    "    if add_noise:\n",
    "        df['tool_wear'] = df['tool_wear'] + np.random.normal(0, 1, n_points)/add_noise\n",
    "\n",
    "    # Normalize\n",
    "    if normalize:\n",
    "        WEAR_MIN = df['tool_wear'].min() \n",
    "        WEAR_MAX = df['tool_wear'].max()\n",
    "        WEAR_THRESHOLD_NORMALIZED = (wear_threshold-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "        df_normalized = (df-df.min())/(df.max()-df.min())\n",
    "\n",
    "        # df_normalized['ACTION_CODE'] = np.where(df_normalized['tool_wear'] < WEAR_THRESHOLD_NORMALIZED, 0.0, 1.0)\n",
    "        # print(f'Tool wear data imported ({n_points} records). WEAR_THRESHOLD_NORMALIZED: {WEAR_THRESHOLD_NORMALIZED:4.3f}')\n",
    "\n",
    "        df_train = df_normalized.copy(deep=True)\n",
    "\n",
    "        tool_wear = df_normalized['tool_wear']\n",
    "        action_code_normalized = df_normalized['ACTION_CODE']\n",
    "        action_code = df['ACTION_CODE']\n",
    "        df_train['ACTION_CODE'] = df['ACTION_CODE']\n",
    "    else:\n",
    "        df_train = df.copy(deep=True)\n",
    "        tool_wear = df['tool_wear']\n",
    "        action_code = df['ACTION_CODE']\n",
    "\n",
    "    plt.figure(figsize=(10, 2.5))\n",
    "    plt.plot(tool_wear, linewidth=1)\n",
    "\n",
    "    if normalize:\n",
    "        plt.plot(action_code_normalized, linewidth=1)\n",
    "        wear_threshold_return = WEAR_THRESHOLD_NORMALIZED\n",
    "        plt.axhline(y = WEAR_THRESHOLD_NORMALIZED, color = 'r', linestyle = '--', alpha=0.3) \n",
    "    else:\n",
    "        plt.plot(action_code, linewidth=1)\n",
    "        wear_threshold_return = wear_threshold\n",
    "        plt.axhline(y = wear_threshold, color = 'r', linestyle = '--', alpha=0.3) \n",
    "\n",
    "    plt.title(f'Tool wear')\n",
    "    plt.grid(color='lightgray', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return tool_wear, action_code, wear_threshold_return, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Down-sampling. Input data records: 1000. Sampling rate: 1. Expected rows 1000.    Down-sampled to 1000 rows.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool failure RUL threshold at time 950 is 0.006\n"
     ]
    }
   ],
   "source": [
    "tool_wear, action_code, WEAR_THRESHOLD_NORMALIZED, df_train = tool_wear_data(\n",
    "    data_file=DATA_FILE, wear_threshold = WEAR_THRESHOLD,\n",
    "    normalize=False, add_noise=ADD_NOISE, sampling_rate = SAMPLING_RATE)\n",
    "\n",
    "records = len(df_train.index)\n",
    "\n",
    "rul_threshold_record = int(0.95 * records)\n",
    "rul_threshold = df_train.loc[df_train.index[rul_threshold_record], 'RUL']\n",
    "print(f'Tool failure RUL threshold at time {rul_threshold_record} is {rul_threshold:3.3f}')\n",
    "# failure_point = df_train.loc[df_train['ACTION_CODE'] == 1].iloc[0]\n",
    "# TOOL_FAILURE_TIME = failure_point['time']\n",
    "# print(f'Tool failure time-point: {TOOL_FAILURE_TIME:3.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Milling tool environment\n",
    "# V.1.0 04-Aug-2024\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "LAMBDA = 0.01\n",
    "NO_ACTION = 0\n",
    "REPLACE = 1\n",
    "\n",
    "# Information arrays \n",
    "a_time = []\n",
    "a_actions = []\n",
    "a_action_text = []\n",
    "a_rewards = []\n",
    "a_rul = []\n",
    "a_cost = []\n",
    "a_replacements = []\n",
    "a_time_since_last_replacement = []\n",
    "a_action_recommended = []\n",
    "\n",
    "class MT_Env(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, records=0, rul_threshold=0.0):\n",
    "        print(f'\\n -- Milling tool environment initiatlized. Potential records {records}. RUL threshold {rul_threshold:4.3f}')\n",
    "        # Initialize\n",
    "        self.df = None\n",
    "        self.records = records\n",
    "        self.maintenance_cost = 0.0\n",
    "        self.replacement_events = 0\n",
    "        self.time_since_last_replacement = 0\n",
    "        \n",
    "        self.rul_threshold = rul_threshold # Usually 5% from 0.0 i.e. 95th percentile record value from the very end \n",
    "        \n",
    "        # Observation vector: ['timestamp', 'vibration_x', 'vibration_y', 'force_z', 'tool_wear', 'RUL', 'ACTION_CODE']\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.records, # Max records (time)\n",
    "                1.0,          # Max. vibration_x\n",
    "                1.0,          # Max. vibration_y\n",
    "                1.0,          # Max. force_z\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # observation space lower limits\n",
    "        low = np.array(\n",
    "            [\n",
    "                0,            # Min. time\n",
    "                -1.0,         # Min. vibration_x\n",
    "                -1.0,         # Min. vibration_y\n",
    "                -1.0,         # Min. force_z\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Actions - Normal, L1-maintenance, L2-maintenance, Replace\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    ## Add tool wear data-set\n",
    "    def tool_wear_data(self, df):\n",
    "        self.df = df\n",
    "        self.records = len(df.index)\n",
    "        print(f'\\n - Milling tool environment: Tool wear data updated: {self.records}')\n",
    "        \n",
    "    ## Constructing Observations From Environment States\n",
    "    # - Observations are needed for both ``reset`` and ``step``, \n",
    "    # - Create private method ``_get_obs`` that translates the environment’s state into an observation.\n",
    "    # - One can additionally use _get_info (in step and reset) if some auxilliary info. needs to be sent - for e.g. Expert action or Reward      #   info. or even RUL\n",
    "    def _get_observation(self):\n",
    "        if (self.df is not None):\n",
    "            obs_values = np.array([\n",
    "                self.df.loc[self.current_time_step, 'time'],\n",
    "                self.df.loc[self.current_time_step, 'vibration_x'],\n",
    "                self.df.loc[self.current_time_step, 'vibration_y'],\n",
    "                self.df.loc[self.current_time_step, 'force_z']\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            obs_values = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "            \n",
    "        observation = obs_values.flatten()\n",
    "        return observation\n",
    "\n",
    "    # Get the current RUL reading, note this is NOT part of the observation\n",
    "    def _get_auxilliary_info(self):\n",
    "        if (self.df is not None):\n",
    "            # From database extract recommended action\n",
    "            recommended_action = self.df.loc[self.current_time_step, 'ACTION_CODE']\n",
    "            rul = self.df.loc[self.current_time_step, 'RUL']\n",
    "        else:\n",
    "            # No database - use dummy values\n",
    "            recommended_action = 'None'\n",
    "            rul = 0.0\n",
    "\n",
    "        return recommended_action, rul\n",
    "            \n",
    "    ## Reset\n",
    "    # 1. Called to initiate a new episode and when 'Done'\n",
    "    # 2. Assume that the ``step`` method will not be called before ``reset``\n",
    "    # 3. Recommended to use RNG ``self.np_random`` provided by base class\n",
    "    # 4. ** Important ** Must call ``super().reset(seed=seed)`` to correctly seed the RNG -- once done, we can randomly set the\n",
    "    # state of our environment. In our case, we randomly choose the agent’s spatial location of \"tool wear\" \n",
    "    # 5. Must return a tuple of the *initial* observation - use ``_get_observation`` \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the tool wear at a random time (spatial) location from a uniformly random distribution\n",
    "        self.current_time_step = np.random.randint(0, self.records, 1, dtype=int)\n",
    "        observation = self._get_observation()\n",
    "        info = {'reset':'Reset'}\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "    ## Step\n",
    "    # 1. Method the logic environment.\n",
    "    # 2. Accepts an ``action``, computes the state of the environment **after** applying that action\n",
    "    # 3. returns the 5-tuple ``(observation, reward, terminated, truncated, info)``\n",
    "    # 4. Once the new state of the environment has been computed - check terminal state / set rewards\n",
    "    # 5. To gather ``observation`` and ``info``, we can use of ``_get_obs`` and ``_get_info``:\n",
    "\n",
    "    def step(self, action):\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        # Get auxilliary info: current RUL reading (note this is NOT part of the observation) and the expert's recommended action\n",
    "        recommended_action, self.rul = self._get_auxilliary_info()\n",
    "        self.maintenance_cost = 0.0\n",
    "        self.replacement_events += 0\n",
    "        \n",
    "        if self.current_time_step >= self.records:\n",
    "            info = {'Step':'EOF'}\n",
    "            terminated = done = True\n",
    "        elif self.rul <= self.rul_threshold: # Less-than-equal 0 (or near zero)\n",
    "            info = {'Step':'RUL threshold crossed'}\n",
    "            terminated = done = True            \n",
    "        elif action == NO_ACTION: # Normal state\n",
    "            self.current_time_step += 1\n",
    "            # 1% reduction in life\n",
    "            self.maintenance_cost += 0.1\n",
    "            info = {'Step':'None'}\n",
    "        elif action == REPLACE:\n",
    "            self.current_time_step += 1\n",
    "            # Replace the tool - reset to begining - but to a random position in the first 10% time-steps \n",
    "            self.maintenance_cost += 10.0\n",
    "            self.replacement_events += 1\n",
    "            self.time_since_last_replacement = self.current_time_step\n",
    "            print(f' -- Time since last replacement: {self.time_since_last_replacement}')\n",
    "            info = {'Step':'* REPLACE *'}\n",
    "\n",
    "        # Action taken, set reward    \n",
    "        self.reward = (self.current_time_step + 1) / (self.maintenance_cost+LAMBDA)\n",
    "\n",
    "        # Information arrays \n",
    "        a_time.append(self.current_time_step)\n",
    "        a_actions.append(action)\n",
    "        a_action_text.append(recommended_action)\n",
    "        a_rewards.append(self.reward)\n",
    "        a_rul.append(self.rul)\n",
    "        a_cost.append(self.maintenance_cost)\n",
    "        a_replacements.append(self.replacement_events)\n",
    "        a_time_since_last_replacement.append(self.time_since_last_replacement)\n",
    "        a_action_recommended.append(recommended_action)\n",
    "        \n",
    "        # Action taken, reward set for that action, now take in next observation\n",
    "        reward = float(self.reward)\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            print('{0:<20} | RUL: {1:>8.2f} | Cost: {2:>8.2f} | Reward: {3:>12.3f}'.format(action_text, self.rul, self.maintenance_cost, self.reward))\n",
    "\n",
    "        return observation, reward, terminated, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -- Milling tool environment initiatlized. Potential records 1000. RUL threshold 0.006\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "gym.register(\n",
    "    id = 'custom/MillingToolEnv-v0',\n",
    "    entry_point = MT_Env,\n",
    "    max_episode_steps = MILLING_OPERATIONS_MAX,\n",
    ")\n",
    "\n",
    "env_test = gym.make('custom/MillingToolEnv-v0', records=records, rul_threshold=rul_threshold)\n",
    "# Check env. formation \n",
    "check_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - Milling tool environment: Tool wear data updated: 1000\n"
     ]
    }
   ],
   "source": [
    "env_test.tool_wear_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11408\\217686756.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEPISODES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mexpert_ppo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMlpPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexpert_ppo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPISODES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpert_ppo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'PPO Expert reward: {reward}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mtb_log_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"PPO\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[1;32m--> 315\u001b[1;33m         return super().learn(\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    190\u001b[0m                     \u001b[1;31m# Otherwise, clip the actions to avoid out of bound error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                     \u001b[1;31m# as we are sampling from an unbounded Gaussian distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[0mclipped_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mnew_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m# Avoid circular imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[0;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             )\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# convert to SB3 VecEnv api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\monitor.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneeds_reset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtruncated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0msteps\u001b[0m \u001b[0melapsed\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmax\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;34m\"\"\"Steps through the environment with `kwargs`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecked_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11408\\1712202931.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time_step\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Step'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'EOF'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mterminated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrul\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrul_threshold\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Less-than-equal 0 (or near zero)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Step'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'RUL threshold crossed'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[0mterminated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mNO_ACTION\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Normal state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1464\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1467\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "EPISODES = 10\n",
    "expert_ppo = PPO(policy=MlpPolicy, env=env_test)\n",
    "expert_ppo.learn(EPISODES)\n",
    "reward, _ = evaluate_policy(expert_ppo, env_test, 10)\n",
    "print(f'PPO Expert reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DEMONSTRATIONS = 10\n",
    "EPISODES = 200 # Train the Expert for N episodes\n",
    "BATCH_SIZE = 16 \n",
    "LEARNING_RATE = int(1e-3)\n",
    "EVALUATION_ROUNDS = 10\n",
    "\n",
    "# Milling tool env.\n",
    "DATA_FILE = 'PHM_C01.csv'\n",
    "R1, R2, R3 = 2.0, -1.0, -20.0\n",
    "WEAR_THRESHOLD = 0.12 # mm\n",
    "THRESHOLD_FACTOR = 1.0\n",
    "ADD_NOISE = 0 # 0=No noise, Low=1e3, High=1e2 \n",
    "BREAKDOWN_CHANCE = 0 # Recommended: 0.05 = 5%\n",
    "MILLING_OPERATIONS_MAX = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the PdM Milling tool environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read tool-wear data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "WEAR_MIN = df['tool_wear'].min() \n",
    "WEAR_MAX = df['tool_wear'].max()\n",
    "WEAR_THRESHOLD_NORMALIZED = THRESHOLD_FACTOR*(WEAR_THRESHOLD-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "df_normalized = (df-df.min())/(df.max()-df.min())\n",
    "df_normalized['ACTION_CODE'] = np.where(df_normalized['tool_wear'] < WEAR_THRESHOLD_NORMALIZED, 0.0, 1.0)\n",
    "print(f'Tool wear data imported ({len(df.index)} records). WEAR_THRESHOLD_NORMALIZED: {WEAR_THRESHOLD_NORMALIZED:4.3f}')\n",
    "df_train = df_normalized\n",
    "\n",
    "# gym.register(\n",
    "#     id = 'custom/MillingTool-v0',\n",
    "#     entry_point = MillingTool,\n",
    "#     max_episode_steps = MILLING_OPERATIONS_MAX,\n",
    "# )\n",
    "\n",
    "# # Vectorized environment: Use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "# env_kwargs = {'df':df_train, 'wear_threshold':WEAR_THRESHOLD_NORMALIZED, 'max_operations':MILLING_OPERATIONS_MAX,\n",
    "#                'add_noise':ADD_NOISE, 'breakdown_chance':BREAKDOWN_CHANCE, 'R1':R1, 'R2':R2, 'R3':R3}\n",
    "\n",
    "# mt_venv = make_vec_env(\n",
    "#     'custom/MillingTool-v0',\n",
    "#     env_make_kwargs=env_kwargs,\n",
    "#     rng=np.random.default_rng(),\n",
    "#     n_envs=1,\n",
    "#     post_wrappers=[lambda mt_venv, _: RolloutInfoWrapper(mt_venv)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id = 'custom/MillingToolNUAA-v0',\n",
    "    entry_point = MillingToolEnv_NUAA,\n",
    "    max_episode_steps = MILLING_OPERATIONS_MAX,\n",
    ")\n",
    "\n",
    "env_test = gym.make('custom/MillingToolNUAA-v0', df=df_train, max_op_cycles=MILLING_OPERATIONS_MAX, \n",
    "                          tool_failure_thresholds=WEAR_THRESHOLD_NORMALIZED, tool_failure_times=TOOL_FAILURE_TIME)\n",
    "\n",
    "# # Vectorized environment: Use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "# env_kwargs = {'df':df_train, 'max_op_cycles':MILLING_OPERATIONS_MAX, 'tool_failure_thresholds':WEAR_THRESHOLD_NORMALIZED, \n",
    "#               'tool_failure_times':TOOL_FAILURE_TIME}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mt_venv_nuaa = make_vec_env(\n",
    "    'custom/MillingToolNUAA-v0',\n",
    "    env_make_kwargs=env_kwargs,\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=1,\n",
    "    post_wrappers=[lambda mt_venv_nuaa, _: RolloutInfoWrapper(mt_venv_nuaa)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Milling environent \n",
    "- Default method of creating is `env_mt = gym.make('custom/MillingTool-v0')`\n",
    "- This does **not** work with `imitation` IRL\n",
    "- Must create a vectorized env. using `make_vec_env` utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorized environment: Use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "# env_kwargs = {'df':df_train, 'max_op_cycles':MILLING_OPERATIONS_MAX, 'tool_failure_thresholds':WEAR_THRESHOLD_NORMALIZED, \n",
    "#               'tool_failure_times':TOOL_FAILURE_TIME}\n",
    "\n",
    "# mt_venv = make_vec_env(\n",
    "#     'custom/MillingToolNUAA-v0',\n",
    "#     env_make_kwargs=env_kwargs,\n",
    "#     rng=np.random.default_rng(),\n",
    "#     n_envs=1,\n",
    "#     post_wrappers=[lambda mt_venv, _: RolloutInfoWrapper(mt_venv)],\n",
    "# )\n",
    "\n",
    "# # Load the wear data dataframe\n",
    "# # mt_venv.load_df(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The \"Human\" Expert (here we create one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_a2c = A2C('MlpPolicy', env_mt)\n",
    "# expert_a2c.learn(total_timesteps=EPISODES)\n",
    "EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_ppo = PPO(\n",
    "    policy = MlpPolicy,\n",
    "    env = env_test,\n",
    "    # seed = 0,\n",
    "    batch_size = 64,\n",
    "    ent_coef = 0.0,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    n_epochs = 10,\n",
    "    n_steps = 64,\n",
    "    # tensorboard_log=LOG_PATH,\n",
    ")\n",
    "\n",
    "expert_ppo.learn(EPISODES) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check PPO Expert reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, _ = evaluate_policy(expert_ppo, mt_venv_nuaa, EVALUATION_ROUNDS)\n",
    "print(f'PPO Expert reward: {reward}')\n",
    "\n",
    "# reward, _ = evaluate_policy(expert_a2c, env_mt, 10)\n",
    "# print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrations from the the Expert\n",
    "\n",
    "- Use the expert to sample some trajectories.\n",
    "- Flatten them to obtain individual transitions for behavior cloning\n",
    "\n",
    "#### Implementation details: \n",
    "- Use `imitation` utilities - Collect 50 episode rollouts, then flatten them to just the transitions that we need for training.\n",
    "- `rollout` function requires a vectorized environment and needs the `RolloutInfoWrapper` around each of the environments\n",
    "- This is why we passed the `post_wrappers` argument to `make_vec_env` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "rollouts = rollout.rollout(\n",
    "    expert_ppo,\n",
    "    mt_venv_nuaa,\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=SAMPLE_DEMONSTRATIONS),\n",
    "    rng = rng,\n",
    ")\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at what we just generated using those library functions:\n",
    "\n",
    "```print(f\"\"\"The `rollout` function generated a list of {len(rollouts)} {type(rollouts[0])}.\n",
    "After flattening, this list is turned into a {type(transitions)} object containing {len(transitions)} transitions.\n",
    "The transitions object contains arrays for: {', '.join(transitions.__dict__.keys())}.\"\n",
    "\"\"\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "obsvs = []\n",
    "acts = []\n",
    "\n",
    "for n in range(len(rollouts)):\n",
    "    acts.append(transitions.acts[n])\n",
    "    obsvs.append(transitions.obs[n])\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.title('Expert Actions')\n",
    "plt.plot(acts)\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.title('Sampled Observations')\n",
    "plt.plot(obsvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we collected our transitions, it's time to set up our behavior cloning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "# Set new logger\n",
    "tmp_path_irl = f'{PATH}/tensorboard/irl_log/BC/'\n",
    "new_logger_irl = imit_logger.configure(tmp_path_irl, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=mt_venv.observation_space,\n",
    "    action_space=mt_venv_nuaa.action_space,\n",
    "    demonstrations=transitions,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    rng=rng,\n",
    "    device='cpu',\n",
    "    custom_logger = new_logger_irl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the untrained policy only gets poor rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_before_training, _ = evaluate_policy(bc_trainer.policy, mt_venv_nuaa, EVALUATION_ROUNDS)\n",
    "print(f\"Reward before training: {reward_before_training: 4.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Behavioural Cloning (BC) based learning from expert demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%b-%Y  %H:%M:%S\")\n",
    "reward_after_training, _ = evaluate_policy(bc_trainer.policy, mt_venv_nuaa, EVALUATION_ROUNDS)\n",
    "\n",
    "print('-'*120)\n",
    "print(' **** IRL with Imitation Libraries and Milling environment ****')\n",
    "print('-'*120)\n",
    "\n",
    "print(dt_string)\n",
    "print(f'Episodes: {EPISODES}')\n",
    "print(f'Rewards Before: {reward_before_training:5.3f} | After: {reward_after_training:5.3f}')\n",
    "# print(f'Training time: {elapsed_time:5.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import sqil\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "# Set new logger\n",
    "tmp_path_irl = f'{PATH}/tensorboard/irl_log/SQIL/'\n",
    "new_logger_irl = imit_logger.configure(tmp_path_irl, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "sqil_trainer = sqil.SQIL(   \n",
    "    venv = mt_venv_nuaa,\n",
    "    demonstrations = transitions,\n",
    "    policy='MlpPolicy',\n",
    "    # device='cpu',\n",
    "    custom_logger = new_logger_irl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward_before_training, _ = evaluate_policy(sqil_trainer.policy, mt_venv_nuaa, 10)\n",
    "print(f\"Reward before training: {reward_before_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqil_trainer.train(\n",
    "    total_timesteps = 100, # Note: set to 1_000_000 to obtain good results\n",
    ")  \n",
    "reward_after_training, _ = evaluate_policy(sqil_trainer.policy, mt_venv_nuaa, 10)\n",
    "print(f\"Reward after training: {reward_after_training:5.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqil_trainer.train(\n",
    "#     total_timesteps=1_000_000, # Note: set to 1_000_000 to obtain good results\n",
    "# )  \n",
    "# reward_after_training, _ = evaluate_policy(sqil_trainer.policy, mt_venv, 10)\n",
    "# print(f\"Reward after training: {reward_after_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: BaselEnv()])\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step([action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
