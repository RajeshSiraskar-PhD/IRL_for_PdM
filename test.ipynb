{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm # Progress bar\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# RL libraries\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# CH-AI Imitation libraries\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "# Custom libraries\n",
    "# from milling_tool_env import MillingTool\n",
    "from utilities import downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'PHM_C01.csv'\n",
    "WEAR_THRESHOLD = 0.12\n",
    "SAMPLING_RATE = 20\n",
    "ADD_NOISE = 0\n",
    "MILLING_OPERATIONS_MAX = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_wear_data(data_file, wear_threshold, normalize=False, add_noise=False, sampling_rate=1):\n",
    "    ## Read data\n",
    "    df_raw = pd.read_csv(data_file)\n",
    "\n",
    "    df = downsample(df_raw, sampling_rate)\n",
    "\n",
    "    # Reset index as the downsampling disturbs the index and then PPO.learn() fails. Gives a \"Key error\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    n_points = len(df.index)\n",
    "\n",
    "    # 1. Add white noise for robustness\n",
    "    if add_noise:\n",
    "        df['tool_wear'] = df['tool_wear'] + np.random.normal(0, 1, n_points)/add_noise\n",
    "\n",
    "    # Normalize\n",
    "    if normalize:\n",
    "        WEAR_MIN = df['tool_wear'].min() \n",
    "        WEAR_MAX = df['tool_wear'].max()\n",
    "        WEAR_THRESHOLD_NORMALIZED = (wear_threshold-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "        df_normalized = (df-df.min())/(df.max()-df.min())\n",
    "\n",
    "        # df_normalized['ACTION_CODE'] = np.where(df_normalized['tool_wear'] < WEAR_THRESHOLD_NORMALIZED, 0.0, 1.0)\n",
    "        # print(f'Tool wear data imported ({n_points} records). WEAR_THRESHOLD_NORMALIZED: {WEAR_THRESHOLD_NORMALIZED:4.3f}')\n",
    "\n",
    "        df_train = df_normalized.copy(deep=True)\n",
    "\n",
    "        tool_wear = df_normalized['tool_wear']\n",
    "        action_code_normalized = df_normalized['ACTION_CODE']\n",
    "        action_code = df['ACTION_CODE']\n",
    "        df_train['ACTION_CODE'] = df['ACTION_CODE']\n",
    "    else:\n",
    "        df_train = df.copy(deep=True)\n",
    "        tool_wear = df['tool_wear']\n",
    "        action_code = df['ACTION_CODE']\n",
    "\n",
    "    plt.figure(figsize=(10, 2.5))\n",
    "    plt.plot(tool_wear, linewidth=1)\n",
    "\n",
    "    if normalize:\n",
    "        plt.plot(action_code_normalized, linewidth=1)\n",
    "        wear_threshold_return = WEAR_THRESHOLD_NORMALIZED\n",
    "        plt.axhline(y = WEAR_THRESHOLD_NORMALIZED, color = 'r', linestyle = '--', alpha=0.3) \n",
    "    else:\n",
    "        plt.plot(action_code, linewidth=1)\n",
    "        wear_threshold_return = wear_threshold\n",
    "        plt.axhline(y = wear_threshold, color = 'r', linestyle = '--', alpha=0.3) \n",
    "\n",
    "    plt.title(f'Tool wear')\n",
    "    plt.grid(color='lightgray', linestyle='-', linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return tool_wear, action_code, wear_threshold_return, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tool_wear, action_code, WEAR_THRESHOLD_NORMALIZED, df_train = tool_wear_data(\n",
    "    data_file=DATA_FILE, wear_threshold = WEAR_THRESHOLD,\n",
    "    normalize=False, add_noise=ADD_NOISE, sampling_rate = SAMPLING_RATE)\n",
    "\n",
    "records = len(df_train.index)\n",
    "\n",
    "rul_threshold_record = int(0.95 * records)\n",
    "rul_threshold = df_train.loc[df_train.index[rul_threshold_record], 'RUL']\n",
    "print(f'Tool failure RUL threshold at time {rul_threshold_record} is {rul_threshold:3.3f}')\n",
    "# failure_point = df_train.loc[df_train['ACTION_CODE'] == 1].iloc[0]\n",
    "# TOOL_FAILURE_TIME = failure_point['time']\n",
    "# print(f'Tool failure time-point: {TOOL_FAILURE_TIME:3.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "# Milling tool environment\n",
    "# V.1.0 04-Aug-2024\n",
    "# ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "LAMBDA = 0.01\n",
    "NO_ACTION = 0\n",
    "REPLACE = 1\n",
    "\n",
    "# Information arrays \n",
    "a_time = []\n",
    "a_actions = []\n",
    "a_action_text = []\n",
    "a_rewards = []\n",
    "a_rul = []\n",
    "a_cost = []\n",
    "a_replacements = []\n",
    "a_time_since_last_replacement = []\n",
    "a_action_recommended = []\n",
    "\n",
    "class MT_Env(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, records=0, rul_threshold=0.0):\n",
    "        print(f'\\n -- Milling tool environment initiatlized. Potential records {records}. RUL threshold {rul_threshold:4.3f}')\n",
    "        # Initialize\n",
    "        self.df = None\n",
    "        self.current_time_step = 0\n",
    "        self.records = records\n",
    "        self.maintenance_cost = 0.0\n",
    "        self.replacement_events = 0\n",
    "        self.time_since_last_replacement = 0\n",
    "        \n",
    "        self.rul_threshold = rul_threshold # Usually 5% from 0.0 i.e. 95th percentile record value from the very end \n",
    "        \n",
    "        # Observation vector: ['timestamp', 'vibration_x', 'vibration_y', 'force_z', 'tool_wear', 'RUL', 'ACTION_CODE']\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.records, # Max records (time)\n",
    "                1.0,          # Max. vibration_x\n",
    "                1.0,          # Max. vibration_y\n",
    "                1.0,          # Max. force_z\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # observation space lower limits\n",
    "        low = np.array(\n",
    "            [\n",
    "                0,            # Min. time\n",
    "                -1.0,         # Min. vibration_x\n",
    "                -1.0,         # Min. vibration_y\n",
    "                -1.0,         # Min. force_z\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Actions - Normal, L1-maintenance, L2-maintenance, Replace\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    ## Add tool wear data-set\n",
    "    def tool_wear_data(self, df):\n",
    "        self.df = df\n",
    "        self.records = len(df.index)\n",
    "        print(f'\\n - Milling tool environment: Tool wear data updated: {self.records}')\n",
    "        \n",
    "    ## Constructing Observations From Environment States\n",
    "    # - Observations are needed for both ``reset`` and ``step``, \n",
    "    # - Create private method ``_get_obs`` that translates the environment’s state into an observation.\n",
    "    # - One can additionally use _get_info (in step and reset) if some auxilliary info. needs to be sent - for e.g. Expert action or Reward      #   info. or even RUL\n",
    "    def _get_observation(self):\n",
    "        if (self.df is not None):\n",
    "            obs_values = np.array([\n",
    "                self.df.loc[self.current_time_step, 'time'],\n",
    "                self.df.loc[self.current_time_step, 'vibration_x'],\n",
    "                self.df.loc[self.current_time_step, 'vibration_y'],\n",
    "                self.df.loc[self.current_time_step, 'force_z']\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            obs_values = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        \n",
    "        observation = obs_values.flatten()\n",
    "        return observation\n",
    "\n",
    "    # Get the current RUL reading, note this is NOT part of the observation\n",
    "    def _get_auxilliary_info(self):\n",
    "        if (self.df is not None):\n",
    "            # From database extract recommended action\n",
    "            recommended_action = int(self.df.loc[self.current_time_step, 'ACTION_CODE'])\n",
    "            rul =  float(self.df.loc[self.current_time_step, 'RUL'])\n",
    "        else:\n",
    "            # No database - use dummy values\n",
    "            recommended_action = 0\n",
    "            rul = 0.0\n",
    "\n",
    "        return recommended_action, rul\n",
    "            \n",
    "    ## Reset\n",
    "    # 1. Called to initiate a new episode and when 'Done'\n",
    "    # 2. Assume that the ``step`` method will not be called before ``reset``\n",
    "    # 3. Recommended to use RNG ``self.np_random`` provided by base class\n",
    "    # 4. ** Important ** Must call ``super().reset(seed=seed)`` to correctly seed the RNG -- once done, we can randomly set the\n",
    "    # state of our environment. In our case, we randomly choose the agent’s spatial location of \"tool wear\" \n",
    "    # 5. Must return a tuple of the *initial* observation - use ``_get_observation`` \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the tool wear at a random time (spatial) location from a uniformly random distribution\n",
    "        self.current_time_step = np.random.randint(0, self.records, 1, dtype=int)\n",
    "        observation = self._get_observation()\n",
    "        info = {'reset':'Reset'}\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "    ## Step\n",
    "    # 1. Method the logic environment.\n",
    "    # 2. Accepts an ``action``, computes the state of the environment **after** applying that action\n",
    "    # 3. returns the 5-tuple ``(observation, reward, terminated, truncated, info)``\n",
    "    # 4. Once the new state of the environment has been computed - check terminal state / set rewards\n",
    "    # 5. To gather ``observation`` and ``info``, we can use of ``_get_obs`` and ``_get_info``:\n",
    "\n",
    "    def step(self, action):\n",
    "        terminated = False\n",
    "        reward = 0.0\n",
    "        info = {'Step':'-'}\n",
    "        # Get auxilliary info: current RUL reading (note this is NOT part of the observation) and the expert's recommended action\n",
    "        recommended_action, self.rul = self._get_auxilliary_info()\n",
    "        self.maintenance_cost = 0.0\n",
    "        self.replacement_events += 0\n",
    "        \n",
    "        if self.current_time_step >= self.records:\n",
    "            terminated = done = True\n",
    "            info = {'Step':'EOF'}\n",
    "        elif self.rul <= self.rul_threshold: # Less-than-equal 0 (or near zero)\n",
    "            terminated = done = True            \n",
    "            info = {'Step':'RUL threshold crossed'}\n",
    "        elif action == NO_ACTION: # Normal state\n",
    "            self.current_time_step += 1\n",
    "            # 1% reduction in life\n",
    "            self.maintenance_cost += 0.1\n",
    "            info = {'Step':'None'}\n",
    "        elif action == REPLACE:\n",
    "            self.current_time_step += 1\n",
    "            # Replace the tool - reset to begining - but to a random position in the first 10% time-steps \n",
    "            self.maintenance_cost += 10.0\n",
    "            self.replacement_events += 1\n",
    "            self.time_since_last_replacement = self.current_time_step\n",
    "            info = {'Step':'* REPLACE *'}\n",
    "\n",
    "        # Action taken, set reward    \n",
    "        self.reward = (self.current_time_step + 1) / (self.maintenance_cost+LAMBDA)\n",
    "\n",
    "        # Information arrays \n",
    "        a_time.append(self.current_time_step)\n",
    "        a_actions.append(action)\n",
    "        a_action_text.append(recommended_action)\n",
    "        a_rewards.append(self.reward)\n",
    "        a_rul.append(self.rul)\n",
    "        a_cost.append(self.maintenance_cost)\n",
    "        a_replacements.append(self.replacement_events)\n",
    "        a_time_since_last_replacement.append(self.time_since_last_replacement)\n",
    "        a_action_recommended.append(recommended_action)\n",
    "        \n",
    "        # Action taken, reward set for that action, now take in next observation\n",
    "        reward = float(self.reward)\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            print('{0:<20} | RUL: {1:>8.2f} | Cost: {2:>8.2f} | Reward: {3:>12.3f}'.format(action_text, self.rul, self.maintenance_cost, self.reward))\n",
    "\n",
    "        return observation, reward, terminated, False, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "gym.register(\n",
    "    id = 'custom/MillingToolEnv-v0',\n",
    "    entry_point = MT_Env,\n",
    "    max_episode_steps = MILLING_OPERATIONS_MAX,\n",
    ")\n",
    "\n",
    "env_test = gym.make('custom/MillingToolEnv-v0', records=records, rul_threshold=rul_threshold)\n",
    "# Check env. formation \n",
    "check_env(env_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.tool_wear_data(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m expert_ppo \u001b[38;5;241m=\u001b[39m PPO(policy\u001b[38;5;241m=\u001b[39mMlpPolicy, env\u001b[38;5;241m=\u001b[39menv_test)\n\u001b[0;32m      3\u001b[0m expert_ppo\u001b[38;5;241m.\u001b[39mlearn(EPISODES)\n\u001b[1;32m----> 4\u001b[0m reward, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpert_ppo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO Expert reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:553\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    535\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\policies.py:363\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[1;32m--> 363\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    366\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\policies.py:274\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mobs_as_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_tensor, vectorized_env\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\utils.py:483\u001b[0m, in \u001b[0;36mobs_as_tensor\u001b[1;34m(obs, device)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03mMoves the observation to the given device.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m:return: PyTorch tensor of the observation on a desired device.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m--> 483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: th\u001b[38;5;241m.\u001b[39mas_tensor(_obs, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m (key, _obs) \u001b[38;5;129;01min\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPISODES = 10\n",
    "expert_ppo = PPO(policy=MlpPolicy, env=env_test)\n",
    "expert_ppo.learn(EPISODES)\n",
    "reward, _ = evaluate_policy(expert_ppo, env_test, 10)\n",
    "print(f'PPO Expert reward: {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DEMONSTRATIONS = 10\n",
    "EPISODES = 200 # Train the Expert for N episodes\n",
    "BATCH_SIZE = 16 \n",
    "LEARNING_RATE = int(1e-3)\n",
    "EVALUATION_ROUNDS = 10\n",
    "\n",
    "# Milling tool env.\n",
    "DATA_FILE = 'PHM_C01.csv'\n",
    "R1, R2, R3 = 2.0, -1.0, -20.0\n",
    "WEAR_THRESHOLD = 0.12 # mm\n",
    "THRESHOLD_FACTOR = 1.0\n",
    "ADD_NOISE = 0 # 0=No noise, Low=1e3, High=1e2 \n",
    "BREAKDOWN_CHANCE = 0 # Recommended: 0.05 = 5%\n",
    "MILLING_OPERATIONS_MAX = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating the PdM Milling tool environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read tool-wear data\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "WEAR_MIN = df['tool_wear'].min() \n",
    "WEAR_MAX = df['tool_wear'].max()\n",
    "WEAR_THRESHOLD_NORMALIZED = THRESHOLD_FACTOR*(WEAR_THRESHOLD-WEAR_MIN)/(WEAR_MAX-WEAR_MIN)\n",
    "df_normalized = (df-df.min())/(df.max()-df.min())\n",
    "df_normalized['ACTION_CODE'] = np.where(df_normalized['tool_wear'] < WEAR_THRESHOLD_NORMALIZED, 0.0, 1.0)\n",
    "print(f'Tool wear data imported ({len(df.index)} records). WEAR_THRESHOLD_NORMALIZED: {WEAR_THRESHOLD_NORMALIZED:4.3f}')\n",
    "df_train = df_normalized\n",
    "\n",
    "# gym.register(\n",
    "#     id = 'custom/MillingTool-v0',\n",
    "#     entry_point = MillingTool,\n",
    "#     max_episode_steps = MILLING_OPERATIONS_MAX,\n",
    "# )\n",
    "\n",
    "# # Vectorized environment: Use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "# env_kwargs = {'df':df_train, 'wear_threshold':WEAR_THRESHOLD_NORMALIZED, 'max_operations':MILLING_OPERATIONS_MAX,\n",
    "#                'add_noise':ADD_NOISE, 'breakdown_chance':BREAKDOWN_CHANCE, 'R1':R1, 'R2':R2, 'R3':R3}\n",
    "\n",
    "# mt_venv = make_vec_env(\n",
    "#     'custom/MillingTool-v0',\n",
    "#     env_make_kwargs=env_kwargs,\n",
    "#     rng=np.random.default_rng(),\n",
    "#     n_envs=1,\n",
    "#     post_wrappers=[lambda mt_venv, _: RolloutInfoWrapper(mt_venv)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id = 'custom/MillingToolNUAA-v0',\n",
    "    entry_point = MillingToolEnv_NUAA,\n",
    "    max_episode_steps = MILLING_OPERATIONS_MAX,\n",
    ")\n",
    "\n",
    "env_test = gym.make('custom/MillingToolNUAA-v0', df=df_train, max_op_cycles=MILLING_OPERATIONS_MAX, \n",
    "                          tool_failure_thresholds=WEAR_THRESHOLD_NORMALIZED, tool_failure_times=TOOL_FAILURE_TIME)\n",
    "\n",
    "# # Vectorized environment: Use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "# env_kwargs = {'df':df_train, 'max_op_cycles':MILLING_OPERATIONS_MAX, 'tool_failure_thresholds':WEAR_THRESHOLD_NORMALIZED, \n",
    "#               'tool_failure_times':TOOL_FAILURE_TIME}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mt_venv_nuaa = make_vec_env(\n",
    "    'custom/MillingToolNUAA-v0',\n",
    "    env_make_kwargs=env_kwargs,\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=1,\n",
    "    post_wrappers=[lambda mt_venv_nuaa, _: RolloutInfoWrapper(mt_venv_nuaa)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Milling environent \n",
    "- Default method of creating is `env_mt = gym.make('custom/MillingTool-v0')`\n",
    "- This does **not** work with `imitation` IRL\n",
    "- Must create a vectorized env. using `make_vec_env` utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Vectorized environment: Use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "# env_kwargs = {'df':df_train, 'max_op_cycles':MILLING_OPERATIONS_MAX, 'tool_failure_thresholds':WEAR_THRESHOLD_NORMALIZED, \n",
    "#               'tool_failure_times':TOOL_FAILURE_TIME}\n",
    "\n",
    "# mt_venv = make_vec_env(\n",
    "#     'custom/MillingToolNUAA-v0',\n",
    "#     env_make_kwargs=env_kwargs,\n",
    "#     rng=np.random.default_rng(),\n",
    "#     n_envs=1,\n",
    "#     post_wrappers=[lambda mt_venv, _: RolloutInfoWrapper(mt_venv)],\n",
    "# )\n",
    "\n",
    "# # Load the wear data dataframe\n",
    "# # mt_venv.load_df(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The \"Human\" Expert (here we create one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert_a2c = A2C('MlpPolicy', env_mt)\n",
    "# expert_a2c.learn(total_timesteps=EPISODES)\n",
    "EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_ppo = PPO(\n",
    "    policy = MlpPolicy,\n",
    "    env = env_test,\n",
    "    # seed = 0,\n",
    "    batch_size = 64,\n",
    "    ent_coef = 0.0,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    n_epochs = 10,\n",
    "    n_steps = 64,\n",
    "    # tensorboard_log=LOG_PATH,\n",
    ")\n",
    "\n",
    "expert_ppo.learn(EPISODES) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check PPO Expert reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, _ = evaluate_policy(expert_ppo, mt_venv_nuaa, EVALUATION_ROUNDS)\n",
    "print(f'PPO Expert reward: {reward}')\n",
    "\n",
    "# reward, _ = evaluate_policy(expert_a2c, env_mt, 10)\n",
    "# print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrations from the the Expert\n",
    "\n",
    "- Use the expert to sample some trajectories.\n",
    "- Flatten them to obtain individual transitions for behavior cloning\n",
    "\n",
    "#### Implementation details: \n",
    "- Use `imitation` utilities - Collect 50 episode rollouts, then flatten them to just the transitions that we need for training.\n",
    "- `rollout` function requires a vectorized environment and needs the `RolloutInfoWrapper` around each of the environments\n",
    "- This is why we passed the `post_wrappers` argument to `make_vec_env` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "rollouts = rollout.rollout(\n",
    "    expert_ppo,\n",
    "    mt_venv_nuaa,\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=SAMPLE_DEMONSTRATIONS),\n",
    "    rng = rng,\n",
    ")\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at what we just generated using those library functions:\n",
    "\n",
    "```print(f\"\"\"The `rollout` function generated a list of {len(rollouts)} {type(rollouts[0])}.\n",
    "After flattening, this list is turned into a {type(transitions)} object containing {len(transitions)} transitions.\n",
    "The transitions object contains arrays for: {', '.join(transitions.__dict__.keys())}.\"\n",
    "\"\"\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "obsvs = []\n",
    "acts = []\n",
    "\n",
    "for n in range(len(rollouts)):\n",
    "    acts.append(transitions.acts[n])\n",
    "    obsvs.append(transitions.obs[n])\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.title('Expert Actions')\n",
    "plt.plot(acts)\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.title('Sampled Observations')\n",
    "plt.plot(obsvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we collected our transitions, it's time to set up our behavior cloning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "# Set new logger\n",
    "tmp_path_irl = f'{PATH}/tensorboard/irl_log/BC/'\n",
    "new_logger_irl = imit_logger.configure(tmp_path_irl, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=mt_venv.observation_space,\n",
    "    action_space=mt_venv_nuaa.action_space,\n",
    "    demonstrations=transitions,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    rng=rng,\n",
    "    device='cpu',\n",
    "    custom_logger = new_logger_irl\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the untrained policy only gets poor rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_before_training, _ = evaluate_policy(bc_trainer.policy, mt_venv_nuaa, EVALUATION_ROUNDS)\n",
    "print(f\"Reward before training: {reward_before_training: 4.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Behavioural Cloning (BC) based learning from expert demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%b-%Y  %H:%M:%S\")\n",
    "reward_after_training, _ = evaluate_policy(bc_trainer.policy, mt_venv_nuaa, EVALUATION_ROUNDS)\n",
    "\n",
    "print('-'*120)\n",
    "print(' **** IRL with Imitation Libraries and Milling environment ****')\n",
    "print('-'*120)\n",
    "\n",
    "print(dt_string)\n",
    "print(f'Episodes: {EPISODES}')\n",
    "print(f'Rewards Before: {reward_before_training:5.3f} | After: {reward_after_training:5.3f}')\n",
    "# print(f'Training time: {elapsed_time:5.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import sqil\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "# Set new logger\n",
    "tmp_path_irl = f'{PATH}/tensorboard/irl_log/SQIL/'\n",
    "new_logger_irl = imit_logger.configure(tmp_path_irl, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "sqil_trainer = sqil.SQIL(   \n",
    "    venv = mt_venv_nuaa,\n",
    "    demonstrations = transitions,\n",
    "    policy='MlpPolicy',\n",
    "    # device='cpu',\n",
    "    custom_logger = new_logger_irl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward_before_training, _ = evaluate_policy(sqil_trainer.policy, mt_venv_nuaa, 10)\n",
    "print(f\"Reward before training: {reward_before_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqil_trainer.train(\n",
    "    total_timesteps = 100, # Note: set to 1_000_000 to obtain good results\n",
    ")  \n",
    "reward_after_training, _ = evaluate_policy(sqil_trainer.policy, mt_venv_nuaa, 10)\n",
    "print(f\"Reward after training: {reward_after_training:5.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqil_trainer.train(\n",
    "#     total_timesteps=1_000_000, # Note: set to 1_000_000 to obtain good results\n",
    "# )  \n",
    "# reward_after_training, _ = evaluate_policy(sqil_trainer.policy, mt_venv, 10)\n",
    "# print(f\"Reward after training: {reward_after_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = DummyVecEnv([lambda: BaselEnv()])\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step([action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
