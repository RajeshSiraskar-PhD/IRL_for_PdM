{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52177af1-80ca-4d1f-b76c-1157b4e83bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class MT_Env(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, records=0, rul_threshold=0.0):\n",
    "        # Initialize\n",
    "        self.df = None\n",
    "        self.records = records\n",
    "        self.rul_threshold = rul_threshold # Usually 5% from 0.0 i.e. 95th percentile record value from the very end \n",
    "        \n",
    "        # Observation vector: ['timestamp', 'vibration_x', 'vibration_y', 'force_z', 'tool_wear', 'RUL', 'ACTION_CODE']\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.records, # Max records (time)\n",
    "                1.0,          # Max. vibration_x\n",
    "                1.0,          # Max. vibration_y\n",
    "                1.0,          # Max. force_z\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # observation space lower limits\n",
    "        low = np.array(\n",
    "            [\n",
    "                0,            # Min. time\n",
    "                -1.0,         # Min. vibration_x\n",
    "                -1.0,         # Min. vibration_y\n",
    "                -1.0,         # Min. force_z\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Actions - Normal, L1-maintenance, L2-maintenance, Replace\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "    ## Add tool wear data-set\n",
    "    def tool_wear_data(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    ## Constructing Observations From Environment States\n",
    "    # - Observations are needed for both ``reset`` and ``step``, \n",
    "    # - Create private method ``_get_obs`` that translates the environment’s state into an observation.\n",
    "    # - One can additionally use _get_info (in step and reset) if some auxilliary info. needs to be sent - for e.g. Expert action or Reward      #   info. or even RUL\n",
    "    def _get_observation(self):\n",
    "        obs_values = np.array([\n",
    "            self.df.loc[self.current_time_step, 'timestamp'],\n",
    "            self.df.loc[self.current_time_step, 'vibration_x'],\n",
    "            self.df.loc[self.current_time_step, 'vibration_y'],\n",
    "            self.df.loc[self.current_time_step, 'force_z'],\n",
    "        ])\n",
    "        \n",
    "        observation = obs_values.flatten()\n",
    "        return observation\n",
    "\n",
    "    # Get the current RUL reading, note this is NOT part of the observation\n",
    "    def _get_auxilliary_info(self):\n",
    "        # From database extract recommended action\n",
    "        recommended_action = self.df.loc[self.current_time_step, 'ACTION_CODE']\n",
    "        rul = self.df.loc[self.current_time_step, 'rul']\n",
    "        return recommended_action, rul\n",
    "            \n",
    "    ## Reset\n",
    "    # 1. Called to initiate a new episode and when 'Done'\n",
    "    # 2. Assume that the ``step`` method will not be called before ``reset``\n",
    "    # 3. Recommended to use RNG ``self.np_random`` provided by base class\n",
    "    # 4. ** Important ** Must call ``super().reset(seed=seed)`` to correctly seed the RNG -- once done, we can randomly set the\n",
    "    # state of our environment. In our case, we randomly choose the agent’s spatial location of \"tool wear\" \n",
    "    # 5. Must return a tuple of the *initial* observation - use ``_get_observation`` \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the tool wear at a random time (spatial) location from a uniformly random distribution\n",
    "        self.current_time_step = np.random.randint(0, self.records, 1, dtype=int)\n",
    "        observation = self._get_observation(self.current_time_step)\n",
    "        info = 'Reset'  \n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "    ## Step\n",
    "    # 1. Method the logic environment.\n",
    "    # 2. Accepts an ``action``, computes the state of the environment **after** applying that action\n",
    "    # 3. returns the 5-tuple ``(observation, reward, terminated, truncated, info)``\n",
    "    # 4. Once the new state of the environment has been computed - check terminal state / set rewards\n",
    "    # 5. To gather ``observation`` and ``info``, we can use of ``_get_obs`` and ``_get_info``:\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Get auxilliary info: current RUL reading (note this is NOT part of the observation) and the expert's recommended action\n",
    "        recommended_action, self.rul = _get_auxilliary_info()\n",
    "        \n",
    "        if self.current_time_step >= self.records:\n",
    "            info = \"EOF\"\n",
    "            done = True\n",
    "        elif self.rul <= self.rul_threshold: # Less-than-equal 0 (or near zero)\n",
    "            info = \"RUL threshold crossed\"\n",
    "            done = True            \n",
    "        elif action == NO_ACTION: # Normal state\n",
    "            self.current_time_step += 1\n",
    "            # 1% reduction in life\n",
    "            self.maintenance_cost += 0.1            \n",
    "            action_text = info = 'None'\n",
    "        elif action == REPLACE:\n",
    "            self.current_time_step += 1\n",
    "            # Replace the tool - reset to begining - but to a random position in the first 10% time-steps \n",
    "            self.maintenance_cost += 10.0\n",
    "            self.replacement_events += 1\n",
    "            self.time_since_last_replacement = self.current_time_step\n",
    "            print(f' -- Time since last replacement: {self.time_since_last_replacement}')            \n",
    "            action_text = info = '* REPLACE *' \n",
    "\n",
    "        # Action taken, set reward    \n",
    "        self.reward = (self.current_time_step + 1) / (self.maintenance_cost+LAMBDA)\n",
    "\n",
    "        # Information arrays \n",
    "        a_time.append(self.current_time_step)\n",
    "        a_actions.append(action)\n",
    "        a_action_text.append(action_text)\n",
    "        a_rewards.append(self.reward)\n",
    "        a_rul.append(self.rul)\n",
    "        a_cost.append(self.maintenance_cost)\n",
    "        a_replacements.append(self.replacement_events)\n",
    "        a_time_since_last_replacement.append(self.time_since_last_replacement)\n",
    "        a_action_recommended.append(recommended_action)\n",
    "        \n",
    "        # Action taken, reward set for that action, now take in next observation\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            print('{0:<20} | RUL: {1:>8.2f} | Cost: {2:>8.2f} | Reward: {3:>12.3f}'.format(action_text, self.rul, self.maintenance_cost, self.reward))\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbcc664-f98c-4485-a214-5fec7143e85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5790e6bc-af49-49aa-800a-1b03ef3ed5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af89b42b-79cb-486f-ada8-aaca6e873e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
